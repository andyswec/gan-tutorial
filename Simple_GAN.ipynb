{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "G_INPUT_SIZE = 1\n",
    "D_INPUT_SIZE = 1\n",
    "D_OUTPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 11\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10 ** 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# INIT\n",
    "\n",
    "def z_sample(shape):\n",
    "    return np.random.uniform(0, 1, shape)\n",
    "\n",
    "\n",
    "def x_sample(shape):\n",
    "    return np.random.normal(-1, 1, shape)\n",
    "\n",
    "\n",
    "def generator(z):\n",
    "    hidden = fully_connected(z, HIDDEN_SIZE, activation_fn=tf.sigmoid)\n",
    "    output = fully_connected(hidden, D_INPUT_SIZE, activation_fn=tf.sigmoid)\n",
    "    return output\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    hidden = fully_connected(x, HIDDEN_SIZE, activation_fn=tf.sigmoid)\n",
    "    output = fully_connected(hidden, D_OUTPUT_SIZE, activation_fn=tf.sigmoid)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D params count: 8\n",
      "G params count: 4\n",
      "Epoch 0: loss_d=2.024170160293579 loss_g=0.2465161383152008\n",
      "Epoch 10000: loss_d=0.700162947177887 loss_g=0.6980217099189758\n",
      "Epoch 20000: loss_d=0.698911726474762 loss_g=0.6978857517242432\n",
      "Epoch 30000: loss_d=0.6983233094215393 loss_g=0.697760820388794\n",
      "Epoch 40000: loss_d=0.6980656385421753 loss_g=0.697624146938324\n",
      "Epoch 50000: loss_d=0.6978524923324585 loss_g=0.6975095868110657\n",
      "Epoch 60000: loss_d=0.6976476907730103 loss_g=0.6973869204521179\n",
      "Epoch 70000: loss_d=0.6974903345108032 loss_g=0.6972706317901611\n",
      "Epoch 80000: loss_d=0.6973512172698975 loss_g=0.6971679925918579\n",
      "Epoch 90000: loss_d=0.6972470879554749 loss_g=0.6970666646957397\n",
      "Epoch 100000: loss_d=0.6971054673194885 loss_g=0.696976900100708\n",
      "Epoch 110000: loss_d=0.6969719529151917 loss_g=0.6968749761581421\n",
      "Epoch 120000: loss_d=0.6969310641288757 loss_g=0.6967953443527222\n",
      "Epoch 130000: loss_d=0.6967716813087463 loss_g=0.6967225670814514\n",
      "Epoch 140000: loss_d=0.6966955661773682 loss_g=0.6966006755828857\n",
      "Epoch 150000: loss_d=0.696634829044342 loss_g=0.6965534090995789\n",
      "Epoch 160000: loss_d=0.6965305805206299 loss_g=0.6964223980903625\n",
      "Epoch 170000: loss_d=0.6964151263237 loss_g=0.6963958144187927\n",
      "Epoch 180000: loss_d=0.6963871717453003 loss_g=0.6963250637054443\n",
      "Epoch 190000: loss_d=0.6963157057762146 loss_g=0.696254551410675\n",
      "Epoch 200000: loss_d=0.696241021156311 loss_g=0.6962092518806458\n",
      "Epoch 210000: loss_d=0.696136474609375 loss_g=0.6961709856987\n",
      "Epoch 220000: loss_d=0.6961342096328735 loss_g=0.6960806846618652\n",
      "Epoch 230000: loss_d=0.6960603594779968 loss_g=0.6960111260414124\n",
      "Epoch 240000: loss_d=0.6959463357925415 loss_g=0.6959969997406006\n",
      "Epoch 250000: loss_d=0.6959078907966614 loss_g=0.695913553237915\n",
      "Epoch 260000: loss_d=0.6958640217781067 loss_g=0.6958914995193481\n",
      "Epoch 270000: loss_d=0.6958275437355042 loss_g=0.6958210468292236\n",
      "Epoch 280000: loss_d=0.6957759261131287 loss_g=0.6957675218582153\n",
      "Epoch 290000: loss_d=0.6957337260246277 loss_g=0.6957500576972961\n",
      "Epoch 300000: loss_d=0.6957024931907654 loss_g=0.6956475973129272\n",
      "Epoch 310000: loss_d=0.6956674456596375 loss_g=0.6956827044487\n",
      "Epoch 320000: loss_d=0.6956384778022766 loss_g=0.6956301927566528\n",
      "Epoch 330000: loss_d=0.6955880522727966 loss_g=0.695586085319519\n",
      "Epoch 340000: loss_d=0.6955656409263611 loss_g=0.6955333948135376\n",
      "Epoch 350000: loss_d=0.6954936981201172 loss_g=0.6954998970031738\n",
      "Epoch 360000: loss_d=0.6954541802406311 loss_g=0.6954702734947205\n",
      "Epoch 370000: loss_d=0.6954054236412048 loss_g=0.6954710483551025\n",
      "Epoch 380000: loss_d=0.6954098343849182 loss_g=0.6953823566436768\n",
      "Epoch 390000: loss_d=0.6953425407409668 loss_g=0.6954191327095032\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('G'):\n",
    "        z = tf.placeholder(tf.float32, shape=(None, G_INPUT_SIZE))\n",
    "        G = generator(z)\n",
    "\n",
    "    with tf.variable_scope('D') as scope:\n",
    "        x = tf.placeholder(tf.float32, shape=(None, D_INPUT_SIZE))\n",
    "        D1 = discriminator(x)\n",
    "        D2 = discriminator(G)\n",
    "\n",
    "        loss_d = tf.reduce_mean(-tf.log(D1) - tf.log(1 - D2))\n",
    "        loss_g = tf.reduce_mean(-tf.log(D2))\n",
    "\n",
    "        trainable_vars = tf.trainable_variables()\n",
    "        d_params = [v for v in trainable_vars if v.name.startswith('D/')]\n",
    "        g_params = [v for v in trainable_vars if v.name.startswith('G/')]\n",
    "        print(\"D params count: {}\".format(len(d_params)))\n",
    "        print(\"G params count: {}\".format(len(g_params)))\n",
    "\n",
    "        opt_d = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss_d)\n",
    "        opt_g = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss_g)\n",
    "        \n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "\n",
    "    for step in range(EPOCHS):\n",
    "        zs = z_sample([BATCH_SIZE, G_INPUT_SIZE])\n",
    "        loss_gs, _ = session.run([loss_g, opt_g], feed_dict={z: zs})\n",
    "\n",
    "        xs = x_sample([BATCH_SIZE, D_INPUT_SIZE])\n",
    "        zs = z_sample([BATCH_SIZE, G_INPUT_SIZE])\n",
    "        loss_ds, _ = session.run([loss_d, opt_d], feed_dict={x: xs, z: zs})\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            print('Epoch {}: loss_d={} loss_g={}'.format(step, loss_ds, loss_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
